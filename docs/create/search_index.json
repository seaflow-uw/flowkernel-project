[["index.html", "Creating the flowkernel R package 1 Preliminaries 1.1 DESCRIPTION file 1.2 Package-level documentation", " Creating the flowkernel R package Farhad de Sousa and Jacob Bien 2025-03-11 1 Preliminaries This document uses litr to define the flowkernel R package. When the index.Rmd file is rendered, the R package is created along with the bookdown you are reading. To do so in RStudio, you can simply open index.Rmd and press “Knit” to render the bookdown (and open _book/index.html to see the result). More generally, in a console you can run the following: litr::render(&quot;index.Rmd&quot;) 1.1 DESCRIPTION file We start by specifying some basic information for the description file: usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;Smoothly Varying Mixture of Gaussians Modeling&quot;, Description = &quot;This package uses kernel-smoothed EM to estimate a smoothly varying mixture of Gaussians model.&quot;, `Authors@R` = c(person( given = &quot;Jacob&quot;, family = &quot;Bien&quot;, email = &quot;jbien@usc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ), person( given = &quot;Farhad&quot;, family = &quot;de Sousa&quot;, email = &quot;fdesousa@usc.edu&quot;, role = c(&quot;aut&quot;) ) ) ) ) usethis::use_mit_license(copyright_holder = &quot;F. Last&quot;) 1.2 Package-level documentation Let’s include some package-level documentation. Besides being user-friendly, it’s also needed because we’ll be using “import from” later. Also, notice that we are importing all of mclust. This is because of this error involving mclustBIC(), which is called when we call Mclust(). #&#39; Smoothly Varying Mixture of Gaussians Modeling #&#39; #&#39; This package uses kernel-smoothed EM to estimate a smoothly varying mixture of Gaussians model. #&#39; #&#39; @docType package #&#39; @import mclust #&#39; @import foreach "],["the-model.html", "2 The model 2.1 Generating data from model 2.2 Visualizing the raw data 2.3 Visualizing data and model", " 2 The model Our interest is in modeling a sequence of scatter plots measured over time. That is, we observe \\(Y_{it}\\in\\mathbb R^d\\) for \\(i=1,\\ldots,n_t\\) and \\(t=1,\\ldots,T\\). In continuous-time flow cytometry data, we notice that this data has two properties: Each scatter plot looks approximately like a mixture of Gaussians. The general clustering structure seen in each scatter plot is slowly varying over time. To model data like this, we wish to fit a smoothly-varying mixture of Gaussians model: \\[ Y_{it}|\\{Z_{it}=k\\}\\sim N_d(\\mu_{kt},\\Sigma_{kt})\\qquad\\mathbb P(Z_{it}=k)=\\pi_{kt} \\] where \\((\\mu_{kt},\\Sigma_{kt},\\pi_{kt})\\) are slowly varying parameters. The actual flow cytometry data that we will be working with will be binned data - the 3-d space is divided into a grid (\\(Y_{it}\\) will represent the location of the \\(i\\)th bin at time \\(t\\)), and for each bin at each point in time, we have a total carbon biomass - \\(C _i^{(t)}\\), which is estimated based on the number of cells in that grid, and the size of each of those cells. We will be interested in looking at how the total biomass for different populations evolve over time. It will be useful to have data generated from this model for testing purposes, so we begin by defining a function for simulating from this model. For now, we give each point a biomass that is drawn from a normal distribution of mean 0.01 and standard deviation 0.001. 2.1 Generating data from model #&#39; Generate data from smoothly-varying mixture of Gaussians model #&#39; #&#39; The smoothly-varying mixture of Gaussians model is defined as follows: #&#39; #&#39; At time t there are n_t points generated as follows: #&#39; #&#39; Y_{it}|\\{Z_{it}=k\\} ~ N_d(mu_{kt},Sigma_{kt}) #&#39; where #&#39; P(Z_{it}=k)=pi_{kt} #&#39; and the parameters (mu_{kt},Sigma_{kt}, pi_{kt}) are all slowly varying in time. #&#39; #&#39; This function generates Y and Z. #&#39; #&#39; @param mu_function a function that maps a vector of times to a T-by-K-by-d #&#39; array of means #&#39; @param Sigma_function a function that maps a vector of times to a #&#39; T-K-by-d-by-d array of covariance matrices #&#39; @param pi_function a function that maps a vector of times to a T-by-K vector #&#39; of probabilities #&#39; @param num_points a T vector of integers giving the number of points n_t to #&#39; generate at each time point t. #&#39; @param start_date the starting date for the generated data. This is created for when running a test requires dates. #&#39; @export generate_smooth_gauss_mix &lt;- function(mu_function, Sigma_function, pi_function, num_points, start_date = &quot;2017-05-28 21:00:00 UTC&quot;) { times &lt;- seq_along(num_points) mu &lt;- mu_function(times) Sigma &lt;- Sigma_function(times) pi &lt;- pi_function(times) K &lt;- ncol(pi) # number of components d &lt;- dim(mu)[3] dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) z &lt;- list() # z[[t]][i] = class of point i at time t y &lt;- list() # y[[t]][i,] = d-vector of point i at time t biomass &lt;- list() # biomass[[t]] = biomass of particles in each bin at time t for (t in times) { z[[t]] &lt;- apply(stats::rmultinom(num_points[t], 1, pi[t, ]) == 1, 2, which) y[[t]] &lt;- matrix(NA, num_points[t], d) biomass[[t]] &lt;- abs(stats::rnorm(num_points[t], mean = 0.01, sd = sqrt(0.001))) for (k in 1:K) { ii &lt;- z[[t]] == k # index of points in component k at time t if (sum(ii) == 0) next if (d == 1) y[[t]][ii, ] &lt;- stats::rnorm(n = sum(ii), mean = mu[t, k, ], sd = Sigma[t, k, , ]) else y[[t]][ii, ] &lt;- mvtnorm::rmvnorm(n = sum(ii), mean = mu[t, k, ], sigma = Sigma[t, k, , ]) } } # Generate dates spaced 1 hour apart start_time &lt;- as.POSIXct(start_date, tz = &quot;UTC&quot;) dates &lt;- seq(start_time, by = &quot;hour&quot;, length.out = length(y)) list(y = y, z = z, mu = mu, Sigma = Sigma, pi = pi, biomass = biomass, dates = dates) } For now, we have We have used two packages in this function, so let’s add these into our package. usethis::use_package(&quot;stats&quot;) usethis::use_package(&quot;mvtnorm&quot;) ## ✔ Adding &#39;stats&#39; to Imports field in DESCRIPTION ## • Refer to functions with `stats::fun()` ## ✔ Adding &#39;mvtnorm&#39; to Imports field in DESCRIPTION ## • Refer to functions with `mvtnorm::fun()` Let’s generate simple examples in the \\(d=1\\) and \\(d=3\\) cases: set.seed(123) d &lt;- 1; K &lt;- 2; ntimes &lt;- 200 ex1 &lt;- list( mu_function = function(times) { mu &lt;- array(NA, c(ntimes, K, d)) mu[, , 1] &lt;- cbind(sin(2 * pi * times / 30), 2) mu }, Sigma_function = function(times) { Sigma &lt;- array(NA, c(ntimes, K, 1, 1)) Sigma[, , 1, 1] &lt;- 0.25 Sigma }, pi_function = function(times) { pi1 &lt;- seq(0.2, 0.8, length=length(times)) cbind(pi1, 1 - pi1) }, num_points = rep(40, ntimes) ) ex1$dat &lt;- generate_smooth_gauss_mix(ex1$mu_function, ex1$Sigma_function, ex1$pi_function, ex1$num_points) d &lt;- 3; K &lt;- 4; ntimes &lt;- 200 ex2 = list( mu_function = function(times) { mu &lt;- array(NA, c(ntimes, K, d)) mu[, , 1] &lt;- cbind(0.5*cos(2 * pi * times / 30), 0.3*sin(2 * pi * times / 30), sin(2 * pi * times / 30), -3) mu[, , 2] = cbind (0.3*sin(2 * pi * times / 30), 2, -1, 0.6*cos(2 * pi * times / 30)) mu[, , 3] = cbind(2, 0.7*cos(2 * pi * times / 30), 0.4*sin(2 * pi * times / 30), 1) mu }, Sigma_function = function(times) { Sigma &lt;- array(NA, c(ntimes, K, d, d)) Sigma[, , 1, 1] &lt;- 0.10 Sigma[, , 1, 2] &lt;- 0 Sigma[, , 1, 3] &lt;- 0 Sigma[, , 2, 1] &lt;- 0 Sigma[, , 2, 2] &lt;- 0.10 Sigma[, , 2, 3] &lt;- 0 Sigma[, , 3, 1] &lt;- 0 Sigma[, , 3, 2] &lt;- 0 Sigma[, , 3, 3] &lt;- 0.10 Sigma }, pi_function = function(times) { pi1 &lt;- seq(0.2, 0.3, length=length(times)) cbind(pi1, pi1, 2*pi1/3, 1- (2*pi1 + 2*pi1/3)) }, num_points = rep(150, ntimes) ) ex2$dat &lt;- generate_smooth_gauss_mix(ex2$mu_function, ex2$Sigma_function, ex2$pi_function, ex2$num_points) 2.2 Visualizing the raw data Let’s make a function for visualizing the data in the one-dimensional and three-dimensional cases. library(magrittr) # we&#39;ll be using the pipe in this document The function will take as input the following argument: ###&quot;y-param&quot;### #&#39; @param y length T list with `y[[t]]` being a n_t-by-d matrix We define this bit of documentation in its own code chunk so that it can be easily reused since multiple functions in the package take it as input. #&#39; Plot raw data #&#39; &lt;&lt;y-param&gt;&gt; #&#39; #&#39; @export plot_data &lt;- function(y) { d &lt;- ncol(y[[1]]) if (d == 1){ y_label &lt;- ifelse(is.null(colnames(y[[1]])), &quot;V1&quot;, colnames(y[[1]])) fig &lt;- purrr::map_dfr(y, ~ tibble::tibble(y = .x), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) %&gt;% ggplot2::ggplot(ggplot2::aes(x = .data$time, y = .data$y)) + ggplot2::geom_point(alpha = 0.2) + ggplot2::labs(x = &quot;Time&quot;, y = y_label, title = &quot;Raw Data&quot;) } else if (d == 3){ d &lt;- ncol(y[[1]]) max_val &lt;- list() max_val_time &lt;- list() min_val = list() min_val_time = list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y = unname(y) # Determine axis labels if (is.null(colnames(y[[1]]))) { x_label &lt;- &quot;V1&quot; y_label &lt;- &quot;V2&quot; z_label &lt;- &quot;V3&quot; } else { x_label &lt;- colnames(y[[1]])[1] y_label &lt;- colnames(y[[1]])[2] z_label &lt;- colnames(y[[1]])[3] } y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) y$time = as.integer(y$time) fig &lt;- plotly::plot_ly( data = y, x = ~x, y = ~y, z = ~z, type = &quot;scatter3d&quot;, frame = ~time, mode = &quot;markers&quot;, size = 80, colors = colorRamp(c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;cyan&quot;, &quot;magenta&quot;, &quot;brown&quot;, &quot;gray&quot;, &quot;darkgreen&quot;, &quot;darkblue&quot;, &quot;darkred&quot;, &quot;darkorange&quot;))) %&gt;% plotly::layout(title = &#39;Raw Data&#39;, scene = list( xaxis = list(range = c(1.1 * min_val_time[[1]], 1.1 * max_val_time[[1]]), title = x_label), yaxis = list(range = c(1.1 * min_val_time[[2]], 1.1 * max_val_time[[2]]), title = y_label), zaxis = list(range = c(1.1 * min_val_time[[3]], 1.1 * max_val_time[[3]]), title = z_label), aspectmode = &quot;manual&quot;, aspectratio = list(x = 1, y = 1, z = 1) # Specify the fixed aspect ratio )) } return(fig) } We’ve used some functions from other packages, so let’s include those in our package: usethis::use_pipe() usethis::use_package(&quot;purrr&quot;) usethis::use_package(&quot;tibble&quot;) usethis::use_package(&quot;dplyr&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_import_from(&quot;rlang&quot;, &quot;.data&quot;) usethis::use_package(&quot;plotly&quot;) usethis::use_package(&quot;grDevices&quot;) ## ✔ Adding &#39;magrittr&#39; to Imports field in DESCRIPTION ## ✔ Writing &#39;R/utils-pipe.R&#39; ## • Run `devtools::document()` to update &#39;NAMESPACE&#39; ## ✔ Adding &#39;purrr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `purrr::fun()` ## ✔ Adding &#39;tibble&#39; to Imports field in DESCRIPTION ## • Refer to functions with `tibble::fun()` ## ✔ Adding &#39;dplyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `dplyr::fun()` ## ✔ Adding &#39;ggplot2&#39; to Imports field in DESCRIPTION ## • Refer to functions with `ggplot2::fun()` ## ✔ Adding &#39;rlang&#39; to Imports field in DESCRIPTION ## ✔ Adding &#39;@importFrom rlang .data&#39; to &#39;R/flowkernel-package.R&#39; ## ✔ Writing &#39;NAMESPACE&#39; ## ✔ Adding &#39;plotly&#39; to Imports field in DESCRIPTION ## • Refer to functions with `plotly::fun()` ## ✔ Adding &#39;grDevices&#39; to Imports field in DESCRIPTION ## • Refer to functions with `grDevices::fun()` Let’s look at our two examples using this plotting function: plot_data(ex1$dat$y) plot_data(ex2$dat$y) 2.3 Visualizing data and model We’ll also want a function for plotting the data with points colored by true (or estimated) cluster. And it will be convenient to also be able to superimpose the true (or estimated) means. The next function does this: #&#39; Plot data colored by cluster assignment with cluster means &lt;&lt;y-param&gt;&gt; #&#39; @param z a length T list with `z[[t]]` being a n_t vector of cluster assignments #&#39; @param mu a T-by-K-by-d array of means #&#39; @param dim an integer which specifies which dimension of the data to plot. Defaults to a vector - `c(1:ncol(y[[1]]))` - that will plot all dimensions together #&#39; @param show_data a Boolean variable which determines whether data points are plotted along with the cluster centers or not. Defaults to `TRUE` #&#39; @export plot_data_and_model &lt;- function(y, z, mu, dim = c(1:ncol(y[[1]])), show_data = TRUE) { d &lt;- ncol(y[[1]]) K &lt;- ncol(mu) ntimes &lt;- length(z) if (d == 1) { y_label &lt;- ifelse(is.null(colnames(y[[1]])), paste0(&quot;V&quot;, dim), colnames(y[[1]])[dim]) df &lt;- data.frame(time = seq_along(mu[, 1, 1])) for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, 1] } plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Model Means&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) fig &lt;- plotly::ggplotly(plt, dynamicTicks = TRUE) if (show_data == TRUE) { dat_df &lt;- purrr::map2_dfr(z, y, ~ tibble::tibble(z = as.factor(.x), y = .y), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) means_df &lt;- tibble::as_tibble(mu[, , 1]) %&gt;% dplyr::mutate(time = dplyr::row_number()) %&gt;% tidyr::pivot_longer(-.data$time, names_to = &quot;cluster&quot;, values_to = &quot;Mean&quot;) %&gt;% dplyr::mutate(cluster = as.factor(cluster)) fig &lt;- ggplot2::ggplot() + ggplot2::geom_line( data = means_df, ggplot2::aes(x = .data$time, y = .data$Mean, group = .data$cluster) ) + ggplot2::geom_point( data = dat_df, ggplot2::aes(x = .data$time, y = .data$y, color = .data$z), alpha = 0.2 ) + ggplot2::labs(x = &quot;Time&quot;, y = y_label, title = &quot;Data and Model&quot;) } } else if (d == 3) { z_dat &lt;- unlist(z) if (isTRUE(all.equal(dim, c(1, 2, 3)))) { if (is.null(colnames(y[[1]]))) { x_label &lt;- &quot;V1&quot; y_label &lt;- &quot;V2&quot; z_label &lt;- &quot;V3&quot; } else { x_label &lt;- colnames(y[[1]])[1] y_label &lt;- colnames(y[[1]])[2] z_label &lt;- colnames(y[[1]])[3] } max_val &lt;- list() max_val_time &lt;- list() min_val &lt;- list() min_val_time &lt;- list() for (dd in seq(d)) { max_val[[dd]] &lt;- sapply(y, function(mat) max(mat[, dd])) max_val_time[[dd]] &lt;- max(max_val[[dd]]) min_val[[dd]] &lt;- sapply(y, function(mat) min(mat[, dd])) min_val_time[[dd]] &lt;- min(min_val[[dd]]) } y &lt;- y %&gt;% purrr::map_dfr(~ tibble::tibble(x = .x[, 1], y = .x[, 2], z = .x[, 3]), .id = &quot;time&quot;) %&gt;% dplyr::mutate(z1 = z_dat) %&gt;% dplyr::mutate(time = as.integer(time)) cluster_data_frames &lt;- vector(&quot;list&quot;, length = K) for (kk in seq(K)) { cluster_mean &lt;- mu[, kk, ] data &lt;- data.frame( X1 = cluster_mean[, 1], X2 = cluster_mean[, 2], X3 = cluster_mean[, 3], time = 1:ntimes ) cluster_data_frames[[kk]] &lt;- data } if (show_data == FALSE) { fig &lt;- plotly::plot_ly(colors = colorRamp(c(&quot;blue&quot;, &quot;orange&quot;, &quot;red&quot;))) %&gt;% plotly::layout(title = &#39;Model Means&#39;) } else { fig &lt;- y %&gt;% plotly::plot_ly( x = ~x, y = ~y, z = ~z, color = ~z1, type = &quot;scatter3d&quot;, frame = ~time, mode = &quot;markers&quot;, size = 80, colors = colorRamp(c(&quot;blue&quot;, &quot;orange&quot;, &quot;red&quot;)) ) %&gt;% plotly::layout(title = &#39;Data and Model&#39;) updatemenus &lt;- list( list( active = 0, type = &#39;buttons&#39;, buttons = list( list( label = &quot;Data Points&quot;, method = &quot;update&quot;, args = list(list(visible = c(TRUE, rep(c(TRUE, TRUE), K)))) ), list( label = &quot;No Data Points&quot;, method = &quot;update&quot;, args = list(list(visible = c(FALSE, rep(c(TRUE, TRUE), K)))) ) ) ) ) } for (kk in seq(K)) { fig &lt;- fig %&gt;% plotly::add_markers(data = cluster_data_frames[[kk]], x = ~X1, y = ~X2, z = ~X3, color = as.factor(kk), size = 120, frame = ~time) if (show_data == TRUE) { fig &lt;- fig %&gt;% plotly::layout(updatemenus = updatemenus) } } fig &lt;- fig %&gt;% plotly::layout(scene = list( xaxis = list(title = x_label, range = c(1.1 * min_val_time[[1]], 1.1 * max_val_time[[1]])), yaxis = list(title = y_label, range = c(1.1 * min_val_time[[2]], 1.1 * max_val_time[[2]])), zaxis = list(title = z_label, range = c(1.1 * min_val_time[[3]], 1.1 * max_val_time[[3]])), aspectmode = &quot;manual&quot;, # Set aspect ratio to manual aspectratio = list(x = 1, y = 1, z = 1) # Specify the fixed aspect ratio )) } else { y_label &lt;- ifelse(is.null(colnames(y[[1]])), paste0(&quot;V&quot;, dim), colnames(y[[1]])[dim]) df &lt;- data.frame(time = seq_along(mu[, 1, 1])) for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, dim] } plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) fig &lt;- plotly::ggplotly(plt, dynamicTicks = TRUE) if (show_data == TRUE) { y &lt;- purrr::map(y, ~ .x[, dim, drop = FALSE]) dat_df &lt;- purrr::map2_dfr(z, y, ~ tibble::tibble(z = as.factor(.x), y = .y), .id = &quot;time&quot;) %&gt;% dplyr::mutate(time = as.numeric(.data$time)) means_df &lt;- tibble::as_tibble(mu[, , dim]) %&gt;% dplyr::mutate(time = dplyr::row_number()) %&gt;% tidyr::pivot_longer(-.data$time, names_to = &quot;cluster&quot;, values_to = &quot;Mean&quot;) %&gt;% dplyr::mutate(cluster = as.factor(cluster)) fig &lt;- ggplot2::ggplot() + ggplot2::geom_line( data = means_df, ggplot2::aes(x = .data$time, y = .data$Mean, group = .data$cluster) ) + ggplot2::geom_point( data = dat_df, ggplot2::aes(x = .data$time, y = .data$y, color = .data$z), alpha = 0.2 ) + ggplot2::labs(x = &quot;Time&quot;, y = y_label, title = &quot;Data and Model&quot;) } } } return(fig) } We used a function from tidyr, so let’s include this package: usethis::use_package(&quot;tidyr&quot;) ## ✔ Adding &#39;tidyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `tidyr::fun()` For now we can use this to visualize the true model, although later this will be useful for visualizing the estimated model. We first plot the 1-d data, with and without the data points: plot_data_and_model(ex1$dat$y, ex1$dat$z, ex1$dat$mu) plot_data_and_model(ex1$dat$y, ex1$dat$z, ex1$dat$mu, show_data = FALSE) We next look at the 3-d example. There are a few different ways in which this function can be used. We can plot the entire data with the cluster means and assignments as a 3-d animation, and we can also choose to leave out the data points themselves and just look at how the cluster centers evolve with time (the entire point with the data points can be too crowded with real data): plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, show_data = FALSE) We can also use this function to plot 1-d projections of the 3-d data, with and without the data points: plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, dim = 1, show_data = FALSE) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, dim = 2) plot_data_and_model(ex2$dat$y, ex2$dat$z, ex2$dat$mu, dim = 2, show_data = FALSE) We would also like to see how the \\(\\pi_{kt}\\)’s evolve with time, and how the biomass of a particular cluster evolves over time. Let’s add these functions to our package. #&#39; Plot cluster populations (pi) over time #&#39; @param pi A T-by-K array, with each row consisting of probabilities that sum to one #&#39; @export plot_pi &lt;- function(pi) { # Create an empty data frame df &lt;- data.frame(time = seq_along(pi[, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(pi)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- pi[, k] } # Create the ggplot with multiple line plots pi_plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(pi), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k)), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Pi&quot;) + ggplot2::ggtitle(&quot;Pi Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(pi))) # Convert ggplot to plotly for interactivity fig &lt;- plotly::ggplotly(pi_plt, dynamicTicks = TRUE) return(fig) } plot_pi(ex2$dat$pi) If we want to plot all three dimensions together, we can use the following function: #&#39; Plot cluster means as 1-d projection over time, with all three dimensions plotted together, in separate plots #&#39; @param mu a T-by-K-by-d array of means #&#39; @export plot_1d_means_triple &lt;- function(y, mu) { # Convert the 3D array into a long data frame for ggplot2 long_df &lt;- data.frame( time = rep(seq_along(mu[, 1, 1]), times = ncol(mu) * dim(mu)[3]), value = as.vector(mu), cluster = factor(rep(rep(1:ncol(mu), each = dim(mu)[1]), times = dim(mu)[3])), dimension = factor(rep(rep(1:dim(mu)[3], each = dim(mu)[1] * ncol(mu)), times = 1)) ) if (is.null(colnames(y[[1]]))) { label_1 &lt;- &quot;V1&quot; label_2 &lt;- &quot;V2&quot; label_3 &lt;- &quot;V3&quot; } else { label_1 &lt;- colnames(y[[1]])[1] label_2 &lt;- colnames(y[[1]])[2] label_3 &lt;- colnames(y[[1]])[3] } # Create the labels for the facets facet_labels &lt;- c(&quot;1&quot; = label_1, &quot;2&quot; = label_2, &quot;3&quot; = label_3) # Calculate the range for each dimension range_df &lt;- long_df %&gt;% dplyr::group_by(dimension) %&gt;% dplyr::summarize(min_value = min(value), max_value = max(value)) %&gt;% dplyr::mutate(padding = 0.1 * (max_value - min_value), y_min = min_value - padding, y_max = max_value + padding) # Merge the range information back to the long data frame long_df &lt;- merge(long_df, range_df, by = &quot;dimension&quot;) # Create the ggplot with facet_grid and custom y-limits for each plot plt &lt;- ggplot2::ggplot(long_df, ggplot2::aes(x = time, y = value, color = cluster)) + ggplot2::geom_line() + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Value&quot;) + ggplot2::facet_grid(dimension ~ ., scales = &quot;free_y&quot;, labeller = ggplot2::labeller(dimension = facet_labels)) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::geom_blank(ggplot2::aes(y = y_min)) + # Add blank points for custom y-limits ggplot2::geom_blank(ggplot2::aes(y = y_max)) return(plt) } Here is an example of running this function: plot_1d_means_triple(ex2$dat$y, ex2$dat$mu) We now add two more plotting functions to our package: one that plots the 1-d means as above, but with the width of each line varying according to pi of each cluster, and another to plot the total biomass over time for each cluster. #&#39; Plot cluster means as 1-d projection over time, with line widths determined by pi #&#39; @param mu a T-by-K-by-d array of means #&#39; @param pi A T-by-K array, with each row consisting of probabilities that sum to one #&#39; @export plot_1d_means_with_width &lt;- function(y, mu, pi, dim = 1) { y &lt;- purrr::map(y, ~ .x[, dim, drop = FALSE]) y_label &lt;- ifelse(is.null(colnames(y[[1]])), paste0(&quot;V&quot;, dim), colnames(y[[1]])[dim]) # Create an empty data frame df &lt;- data.frame(time = seq_along(pi[, 1])) # Use a for loop to append each column to the data frame for (k in 1:ncol(mu)) { col_name &lt;- paste(&quot;Cluster&quot;, k) df[[col_name]] &lt;- mu[, k, dim] } # Create the ggplot with multiple line plots fig &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time)) + lapply(1:ncol(mu), function(k) { ggplot2::geom_line(ggplot2::aes(y = df[, k + 1], color = paste(&quot;Cluster&quot;, k), linewidth = pi[, k]), linetype = &quot;solid&quot;) }) + ggplot2::labs(x = &quot;Time&quot;, y = y_label) + ggplot2::ggtitle(&quot;Cluster Means Over Time&quot;) + ggplot2::scale_color_manual(name = &quot;Cluster&quot;, values = rainbow(ncol(mu))) + ggplot2::guides(linewidth = &quot;none&quot;) # To remove the linewidth legend return(fig) } plot_1d_means_with_width(ex2$dat$y, ex2$dat$mu, ex2$dat$pi, dim = 1) Let’s also create a function to plot the biomass of each cluster over time. We will run this function later when we have responsibilities defined: #&#39; Plot biomass over time for each cluster #&#39; @param biomass A list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing the biomass (or count) of particles in each bin #&#39; @param resp length T list with `y[[t]]` being a n_t-by-K matrix #&#39; @export plot_biomass &lt;- function(biomass, resp) { K &lt;- ncol(resp[[1]]) ntimes &lt;- length(resp) # Initialize a list to hold data frames for each cluster data_list &lt;- vector(&quot;list&quot;, K) for (k in 1:K) { cluster_biomass &lt;- sapply(1:ntimes, function(tt) sum(resp[[tt]][, k] * biomass[[tt]])) data_list[[k]] &lt;- data.frame(time = seq_along(cluster_biomass), Cluster = paste(&quot;Cluster&quot;, k), Biomass = cluster_biomass) } # Combine all the data frames into one df &lt;- do.call(rbind, data_list) # Create the ggplot with multiple line plots plt &lt;- ggplot2::ggplot(df, ggplot2::aes(x = time, y = Biomass, color = Cluster)) + ggplot2::geom_line() + ggplot2::labs(x = &quot;Time&quot;, y = &quot;Cluster Biomass&quot;) + ggplot2::ggtitle(&quot;Cluster Biomass Over Time&quot;) # Convert ggplot to plotly for interactivity pi_plotly &lt;- plotly::ggplotly(plt, dynamicTicks = TRUE) return(pi_plotly) } "],["the-method.html", "3 The method 3.1 E-step 3.2 M-step 3.3 Initialization 3.4 Trying out the method 3.5 Cross-Validation", " 3 The method Given a kernel \\(w_h(t)\\) with bandwidth \\(h\\), we will be considering the following EM-inspired algorithm. Here’s a high-level look at the algorithm. We’ll discuss the initialization, E-step, and M-step in the next subsections. #&#39; A Kernel-smoothed EM algorithm that fits a mixture of Gaussians that evolves over time. The larger any of the three bandwidths are, the smoother the corresponding parameter will vary. #&#39; #&#39; Element t in the list `y`, `y[[t]]`, is an n_t-by-d matrix with the coordinates of every point (or bin) at time t given in each row. `biomass[[t]]` is a numeric vector of length n_t, where the ith entry of the vector is the biomass for the bin whose coordinates are given in the ith row of `y[[t]]`. #&#39; #&#39; The initial_fit parameter is a list of initial parameter values that is generated from the initialization functions in the package. It contains initial values at all times for mu, Sigma, and pi, as well as estimates for the responsibilities and cluster membership. Currently, for our cluster membership estimates (z estimate - zest), each bin is assigned to the cluster that is most responsible for it. #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of clusters/populations #&#39; @param hmu bandwidth for mu parameter #&#39; @param hSigma bandwidth for Sigma parameter #&#39; @param hpi bandwidth for pi parameter #&#39; @param dates A vector of POSIXct (or POSIXt) date-time objects of length T, where each element represents the timestamp corresponding to the t-th observation. If provided, these dates are converted to numeric values and rescaled (by dividing by 3600) for time-based smoothing operations. If omitted, the function defaults to using the sequence of observation indices. #&#39; @param num_iter number of iterations of EM to perform #&#39; @param biomass list of length T, where each element `biomass[[t]]` is a #&#39; numeric vector of length n_t containing the biomass (or count) of particles #&#39; in each bin #&#39; @param initial_fit a list of starting values for the parameters, responsibilities, and estimated cluster assignments #&#39; @export kernel_em &lt;- function (y, K, hmu, hSigma, hpi, dates = NULL, num_iter = 10, biomass = default_biomass(y), initial_fit = init_const(y, K, 50, 50)) { num_times &lt;- length(y) d &lt;- ncol(y[[1]]) mu &lt;- initial_fit$mu Sigma &lt;- initial_fit$Sigma pi &lt;- initial_fit$pi if (!is.null(dates)){ numeric_dates &lt;- as.numeric(dates) rescaled_dates &lt;- (numeric_dates - min(numeric_dates)) / 3600 } for (l in seq(num_iter)) { &lt;&lt;E-step&gt;&gt; &lt;&lt;M-step&gt;&gt; } zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) dimnames(Sigma) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL, NULL) dimnames(pi) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K)) list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) } To start off our first E-step, we need estimates of \\((\\mu,\\Sigma,\\pi)\\) for every cluster for every time point, which we get from our initialization. Before diving into the initialization, however, let us first look at the main algorithm: 3.1 E-step Given an estimate of \\((\\mu,\\Sigma,\\pi)\\), the E-step computes for each \\(Y_{it}\\) how “responsible” each cluster is for it. In particular, the responsibility vector \\((\\hat\\gamma_{it1},\\ldots,\\hat\\gamma_{itK})\\) is a probability vector. It is computed using Bayes rule: \\[ \\hat\\gamma_{itk}=\\hat{\\mathbb{P}}(Z_{it}=k|Y_{it})=\\frac{\\hat \\pi_{tk}\\phi(Y_{it};\\hat\\mu_{tk},\\hat\\Sigma_{tk})}{\\sum_{\\ell=1}^K\\hat \\pi_{t\\ell}\\phi(Y_{it};\\hat\\mu_{t\\ell },\\hat\\Sigma_{t\\ell})} \\] We will create a function that calculates responsibilities given parameter estimates, as we will need to calculate responsibilities elsewhere too (in the initialization). In the function below, we calculate responsibilities using the log of the densities to help with numerical stability, and we use matrixStats::rowLogSumExps(), which implements the LogSumExp function (also called RealSoftMax) in a stable way. #&#39; Calculates responsibilities for each point (or bin) at each time point, given parameter estimates for all clusters at all times &lt;&lt;y-param&gt;&gt; #&#39; @param mu a T-by-K-by-d array of means #&#39; @param Sigma a T-K-by-d-by-d array of covariance matrices #&#39; @param pi a T-by-K vector of probabilities #&#39; @export calculate_responsibilities &lt;- function(y, mu, Sigma, pi){ resp &lt;- list() # responsibilities gamma[[t]][i, k] log_resp &lt;- list() # log of responsibilities d &lt;- ncol(y[[1]]) K &lt;- ncol(mu) num_times &lt;- length(y) if (d == 1) { for (tt in seq(num_times)) { log_phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { log_phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu[tt, k, 1], sd = sqrt(Sigma[tt, k, 1, 1]), log = TRUE) } log_temp = t(t(log_phi) + log(pi[tt, ])) log_resp[[tt]] = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[tt]] = exp(log_resp[[tt]]) } } else if (d &gt; 1) { for (tt in seq(num_times)) { log_phi &lt;- matrix(NA, nrow(y[[tt]]), K) for (k in seq(K)) { log_phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu[tt, k, ], sigma = Sigma[tt, k, , ], log = TRUE) } log_temp = t(t(log_phi) + log(pi[tt, ])) log_resp[[tt]] = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[tt]] = exp(log_resp[[tt]]) } } return(resp) } ###&quot;E-step&quot;### # E-step: update responsibilities resp &lt;- calculate_responsibilities(y, mu, Sigma, pi) resp_weighted &lt;- purrr::map2(biomass, resp, ~ .y * .x) 3.2 M-step In the M-step, we update the estimates of \\((\\mu,\\Sigma,\\pi)\\): ###&quot;M-step&quot;### # M-step: update estimates of (mu, Sigma, pi) &lt;&lt;M-step-pi&gt;&gt; &lt;&lt;M-step-mu&gt;&gt; &lt;&lt;M-step-Sigma&gt;&gt; We now assume that we are working with binned data, where \\(C_i^{(t)}\\) is the number of particles (or the biomass) at time \\(t\\) in bin \\(i\\), where \\(i\\) goes from \\(1\\) to \\(B\\), the total number of bins. In the case of unbinned data, we take \\(C_i^{(t)} = 1\\) for each point in the data. \\[ \\hat\\gamma_{\\cdot sk}=\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}, \\] which is an estimate of the number of points (or total biomass) in class \\(k\\) at time \\(s\\), and define \\[ \\tilde\\gamma_{\\cdot tk}=\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}, \\] which is a smoothed version of this estimate. Then we estimate \\(\\pi\\) as follows: \\[ \\hat\\pi_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}}=\\frac{\\tilde\\gamma_{\\cdot tk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}} \\] where \\(n_s = \\sum_{i=1}^{B} C_i^{(s)}\\) is the total number of points (or total biomass) at time \\(s\\). For \\(\\mu\\): \\[ \\hat\\mu_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is}}{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\hat\\gamma_{\\cdot sk}} \\] For \\(\\Sigma\\): \\[ \\hat\\Sigma_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top}{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\hat\\gamma_{\\cdot sk}} \\] Each of these quantities involves a summation over \\(i\\) before the smoothing over time. In each case we do the summation over \\(i\\) first so that then all quantities can be expressed as an array (rather than as lists). This should make the smoothing more efficient. 3.2.1 M-step \\(\\pi\\) For \\(\\pi\\) estimation, we compute \\[ \\hat\\gamma_{\\cdot sk}=\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}, \\] And then we compute the kernel smoothed version of this: \\[ \\tilde\\gamma_{\\cdot tk}=\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}, \\] We are then ready to compute the following: \\[ \\hat\\pi_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\pi}(t-s)\\hat\\gamma_{\\cdot sk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}}=\\frac{\\tilde\\gamma_{\\cdot tk}}{\\sum_{s=1}^T{w_{h_\\pi}(t-s)n_s}} \\] ###&quot;M-step-pi&quot;### # do summations over i: #form T-by-K matrix summing resp_itk over i resp_sum &lt;- purrr::map(resp_weighted, ~ colSums(.x)) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) if (is.null(dates)){ resp_sum_smooth &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hpi, x.points = 1:length(x))$y ) } else { resp_sum_smooth &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(rescaled_dates, x, bandwidth = hpi, x.points = rescaled_dates)$y ) } pi &lt;- resp_sum_smooth / rowSums(resp_sum_smooth) Here is an example that demonstrates how the ksmooth() function works: xx &lt;- 5 * sin((1:100) / 5) + rnorm(100) plot(xx, type=&quot;o&quot;) lines(stats::ksmooth(1:length(xx), xx, bandwidth = 5, x.points = 1:length(xx)), col=&quot;red&quot;) lines(stats::ksmooth(1:length(xx), xx, bandwidth = 20, x.points = 1:length(xx)), col=&quot;blue&quot;) 3.2.2 M-step \\(\\mu\\) Next, we compute the estimate of \\(\\mu\\). We again first compute the unsmoothed estimate and then apply smoothing to it: \\[ \\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is} \\] This is then used in the following smoothed estimate: \\[ \\hat\\mu_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}Y_{is}}{\\sum_{s=1}^Tw_{h_\\mu}(t-s)\\hat\\gamma_{\\cdot sk}} \\] ###&quot;M-step-mu&quot;### # form T-by-K-by-d array summing resp_itk * Y_ij over i y_sum &lt;- purrr::map2(resp_weighted, y, ~ crossprod(.x, .y)) %&gt;% unlist() %&gt;% array(c(K, d, num_times)) %&gt;% aperm(c(3,1,2)) if (is.null(dates)){ y_sum_smoothed &lt;- apply( y_sum, 2:3, function(x) stats::ksmooth(1:length(x), x, bandwidth = hmu, x.points = 1:length(x))$y ) resp_sum_smooth_mu &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hmu, x.points = 1:length(x))$y ) } else { y_sum_smoothed &lt;- apply( y_sum, 2:3, function(x) stats::ksmooth(rescaled_dates, x, bandwidth = hmu, x.points = rescaled_dates)$y ) resp_sum_smooth_mu &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(rescaled_dates, x, bandwidth = hmu, x.points = rescaled_dates)$y ) } for (j in seq(d)) { mu[, , j] &lt;- y_sum_smoothed[, , j] / resp_sum_smooth_mu } In the above code for y_sum, I convert a list of length \\(T\\), where each list element is a \\(K\\times d\\) matrix, to a \\(T\\times K\\times d\\) array. To verify that this conversion is done correctly, I tried this small example: a &lt;- list(matrix(1:12, 4, 3), matrix(13:24, 4, 3)) a a %&gt;% unlist() %&gt;% array(c(4,3,2)) ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 13 17 21 ## [2,] 14 18 22 ## [3,] 15 19 23 ## [4,] 16 20 24 3.2.3 M-step \\(\\Sigma\\) We start by computing \\[ \\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top \\] and then go on to compute \\[ \\hat\\Sigma_{tk}=\\frac{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\sum_{i=1}^{B}C_i^{(s)}\\hat\\gamma_{isk}(Y_{is}-\\hat\\mu_{sk})(Y_{is}-\\hat\\mu_{sk})^\\top}{\\sum_{s=1}^Tw_{h_\\Sigma}(t-s)\\hat\\gamma_{\\cdot sk}} \\] ###&quot;M-step-Sigma&quot;### # form a T-by-K-by-d-by-d array # summing (Y_it - mu_t)*diag(resp_itk)*(Y_it - mu_t)^T over i mat_sum &lt;- array(NA, c(num_times, K, d, d)) for (tt in seq(num_times)) { yy &lt;- matrix(NA, dim(y[[tt]])[1], d) for (k in seq(K)) { for(dd in seq(d)) { yy [,dd] &lt;- (y[[tt]][, dd] - mu[tt, k, dd]) } mat_sum[tt, k, , ] &lt;- crossprod(yy, yy * resp_weighted[[tt]][, k]) } } if (is.null(dates)){ mat_sum_smoothed &lt;- apply( mat_sum, 2:4, function(x) stats::ksmooth(1:length(x), x, bandwidth = hSigma, x.points = 1:length(x))$y ) resp_sum_smooth_Sigma &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(1:length(x), x, bandwidth = hSigma, x.points = 1:length(x))$y ) } else { mat_sum_smoothed &lt;- apply( mat_sum, 2:4, function(x) stats::ksmooth(rescaled_dates, x, bandwidth = hSigma, x.points = rescaled_dates)$y ) resp_sum_smooth_Sigma &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(rescaled_dates, x, bandwidth = hSigma, x.points = rescaled_dates)$y ) } for (j in seq(d)) for (l in seq(d)) Sigma[, , j, l] &lt;- mat_sum_smoothed[, , j, l] / resp_sum_smooth_Sigma 3.3 Initialization First, we begin with a function that creates a “default biomass” for data that is unbinned (we give each point a biomass (or count) of 1). #&#39; Creates a biomass list of length T, where each element `biomass[[t]]` is a numeric vector of length n_t containing just 1&#39;s. &lt;&lt;y-param&gt;&gt; #&#39; @export default_biomass &lt;- function(y) { biomasslist &lt;- vector(&quot;list&quot;, length(y)) for (i in 1:length(y)) { biomasslist[[i]] &lt;- as.numeric(rep(1, dim(y[[i]])[1])) } return(biomasslist) } We have tried numerous ways of initializing: randomly sampling points for a constant initialization, fitting a separate mixture model to each time point and trying to match clusters using the Hungarian algorithm, having a “chain of EM-algorithms”, with each one initialized by the previous time; and using a Bayesian approach, which is described below. The methods we settled on are the constant initialization and the Bayesian initialization. The kernel-EM algorithm seems to be able to do about the same with both initializations, and so even though the Bayesian approach gives better results, the speed of the constant initialization makes it the practical choice. 3.3.1 Constant Initialization We get a constant initialization by randomly sampling time points, and then from each sampled time point, randomly sampling some points, and fitting a regular EM-algorithm using the mclust package to the randomly chosen points. #&#39; Initialize the Kernel EM-algorithm using constant parameters #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param times_to_sample number of time points to sample #&#39; @param points_to_sample number of bins to sample from each sampled time point #&#39; @export init_const &lt;- function (y, K, times_to_sample = 50, points_to_sample = 50){ num_times &lt;- length(y) d &lt;- ncol(y[[1]]) mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) # Repeatedly call Mclust until it gives a non-NULL fit: init_fit &lt;- NULL while (is.null(init_fit)) { # subsample data: sampled_times &lt;- sample(num_times, times_to_sample, replace=TRUE) if (d == 1) { sample_data &lt;- y[sampled_times] %&gt;% purrr::map(~ .x[sample(nrow(.x), points_to_sample, replace=TRUE)]) %&gt;% unlist() } else { sample_data &lt;- y[sampled_times] %&gt;% purrr::map(~ t(.x[sample(nrow(.x), points_to_sample, replace=TRUE), ])) %&gt;% unlist() %&gt;% matrix(ncol = d, byrow = TRUE) } if (d == 1) { init_fit &lt;- mclust::Mclust(sample_data, G = K, modelNames = &quot;V&quot;) for (tt in seq(num_times)) { mu[tt, , 1] &lt;- init_fit$parameters$mean Sigma[tt, , 1, 1] &lt;- init_fit$parameters$variance$sigmasq pi[tt, ] &lt;- init_fit$parameters$pro } } else if (d &gt; 1) { init_fit &lt;- mclust::Mclust(sample_data, G = K, modelNames = &quot;VVV&quot;) if (is.matrix(init_fit$parameters$mean)){ for (tt in seq(num_times)) { mu[tt, ,] &lt;- t(init_fit$parameters$mean) pi[tt, ] &lt;- init_fit$parameters$pro Sigma[tt, , , ] &lt;- aperm(init_fit$parameters$variance$sigma, c(3,1,2)) } } } } #calculate responsibilities resp &lt;- calculate_responsibilities(y, mu, Sigma, pi) zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) } In the above, we concatenated a list of matrices. Here’s a small example to verify that this is working as intended: list_of_mats &lt;- list(matrix(1:10, 2, 5), matrix(11:25, 3, 5)) list_of_mats list(list_of_mats) %&gt;% purrr::map(~ t(.x)) %&gt;% unlist() %&gt;% matrix(ncol = 5, byrow = TRUE) ## [[1]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 ## ## [[2]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 14 17 20 23 ## [2,] 12 15 18 21 24 ## [3,] 13 16 19 22 25 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 ## [5,] 21 22 23 24 25 Let’s have a look at what the initialization does on our example data. ex1_init_const = init_const(ex1$dat$y, K = 2) plot_data_and_model(ex1$dat$y, ex1_init_const$zest, ex1_init_const$mu) What about the 3-d initialization? ex2_init_const = init_const(ex2$dat$y, K = 4) plot_data_and_model(ex2$dat$y, ex2_init_const$zest, ex2_init_const$mu) Pretty reasonable centers were chosen! If we get a bad initialization by chance (an unlucky random sampling), we can always re-run the constant initialization as it is very cheap. 3.3.2 Bayesian Initialization We’d like to have an initialization method that can vary with time yet can do well even for time points where there is only a small number of particles in a given cluster. The idea is to take a Bayesian approach in which the previous time point serves as the prior for the next time point. The implicit assumption is that the parameters vary sufficiently smoothly that the previous time point’s parameter values are a reasonable estimate of the next time point’s parameter values. For now, we take the priors to be based on the previous time point, but we can also consider priors formed based on all previous time points. Let \\(\\tilde\\gamma_{itk} = C_i^{(t)}\\gamma_{itk}\\), which are “weighted responsibilities”, where each \\(\\gamma_{itk}\\) has been multiplied with the corresponding biomass of bin \\(i\\) at time \\(t\\). We use these weighted responsibilities in our Bayesian approach (can make it unweighted to simplify). 3.3.2.1 M-step \\(\\pi\\) For \\(\\pi_{tk}\\), we assume a Dirichlet prior, getting a posterior mean of: (this specific calculation needs to be double checked) At time \\(t\\) we imagine estimating the number of points in each class \\(k\\), which is \\[ n_t\\hat{\\pi}_{tk}|\\pi_{tk}\\sim\\text{Multinomial}(n_t,\\pi_{t}) \\] where \\(\\hat{\\pi}_{tk},\\pi_{tk}\\) are \\(K\\)-dimensional probability vectors. Our prior takes the previous number of points \\[ \\pi_{tk}\\sim \\text{Dirichlet}(n_{t-1}\\hat{\\pi}_{t-1k}). \\] (Here we are conditioning on everything that happened before time \\(t\\), but omitting this in the notation.) Thus, the posterior is \\[ \\pi_{tk}|n_t\\hat\\pi_{tk}\\sim \\text{Dirichlet}(n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1}) \\] and the posterior mean is thus \\[ \\mathbb{E}[\\pi_{tk}|n_t\\hat\\pi_{tk}]=\\frac{n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1,k}}{\\sum_{k=1}^K[n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1,k}]}=\\frac{n_t\\hat\\pi_{tk}+n_{t-1}\\hat{\\pi}_{t-1,k}}{n_t+n_{t-1}}. \\] In other words, we estimate the vector \\(\\pi_{tk}\\) to be the convex combination \\(\\alpha_t\\hat\\pi_{tk}+(1-\\alpha_t)\\hat\\pi_{t-1k}\\), where \\(\\alpha_t=n_t/(n_t+n_{t-1})\\). Our final estimate is therefore \\[ \\hat{\\pi}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right) + \\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{it-1k}\\right)} {n_{t-1} + n_{t}} \\] where \\(n_t = \\sum_{i=1}^{B} C_i^{(t)}\\) is the total number of points (or total biomass) at time \\(t\\). Here, \\(B\\) is the total number of bins (or points, if the data is unbinned). 3.3.2.2 M-step \\(\\mu\\) For estimating \\(\\mu_{tk}\\), we assume that we known covariance matrix at time \\(t\\): \\(\\hat\\Sigma_{t-1,k}\\) (the previous time’s estimate); and we assume a multivariate normal prior centered around the previous time’s mean, with the scaled covariance matrix: \\[ \\mu_{tk}\\sim\\mathcal{N}(\\hat{\\mu}_{t-1,k}, \\frac{1}{n_{t-1,k}} \\hat\\Sigma_{t-1,k}) \\] We then get the following posterior: \\[ \\mu_{tk}|\\hat{\\mu}_{tk}\\sim\\mathcal{N}(\\frac{n_{tk}\\bar{Y}_{it} + n_{t-1}\\hat{\\mu}_{t-1,k}}{n_{tk}+n_{t-1,k}}, \\frac{1}{n_{t-1,k} + n_{tk}} \\hat\\Sigma_{t-1,k}) \\] where \\(\\bar{Y}_{it} = \\frac{\\sum_{i=1}^{n_{t}} \\tilde\\gamma_{itk}Y_{it}}{\\sum_{i=1}^{n_{t}} \\tilde\\gamma_{itk}}\\) can be thought of as the mean of our data points in cluster \\(k\\) at time \\(t\\). Our estimate for \\(\\mu_{tk}\\) is then \\[ \\hat{\\mu}_{tk} = \\mathbb{E}[\\mu_{tk}|\\hat\\mu_{tk}] = \\frac{ \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk} Y_{it} + \\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right) \\hat{\\mu}_{t-1,k}} { \\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right) + \\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right)} \\] 3.3.2.3 M-step \\(\\Sigma\\) We then estimate \\(\\hat\\Sigma_{tk}\\) in a similar way, assuming a prior that’s an inverse-Wishart with mean \\(\\hat\\Sigma_{t-1,k}\\). Our estimate is: \\[ \\hat{\\Sigma}_{tk} = \\frac{\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right) \\Sigma_{k,t-1} + \\sum_{i=1}^{n_t} \\tilde\\gamma_{itk} \\left(Y_{it} - \\mu_{tk}\\right) \\left(Y_{it} - \\mu_{tk}\\right)^T} {\\left(\\sum_{i=1}^{n_{t-1}} \\tilde\\gamma_{i,t-1,k}\\right) + \\left(\\sum_{i=1}^{n_t} \\tilde\\gamma_{itk}\\right)} \\] A bit more thought is required regarding what multiple iterations (default number of iterations is 1) of this Bayesian EM-algorithm would do (on the second iteration and above, we use \\(\\mu_{kt}\\) from the previous iteration in the place of \\(\\mu_{t-1,k}\\), and similarly for \\(\\Sigma_{tk}\\) and \\(\\pi_{tk}\\)). We also add the option to introduce Laplace smoothing for calculating the responsibilities, to prevent \\(\\pi_{tk}\\) collapsing to 0 over time for some clusters. The Laplace smoothing works by replacing the responsibilities calculation with: \\[ \\hat\\gamma_{itk}=\\hat{\\mathbb{P}}(Z_{it}=k|Y_{it})=\\frac{\\hat \\pi_{tk}\\phi(Y_{it};\\hat\\mu_{tk},\\hat\\Sigma_{tk}) + \\alpha \\min_{c} \\hat \\pi_{tc}\\phi(Y_{it};\\hat\\mu_{tc},\\hat\\Sigma_{tc})}{\\sum_{\\ell=1}^K \\left( \\hat \\pi_{t\\ell}\\phi(Y_{it};\\hat\\mu_{t\\ell },\\hat\\Sigma_{t\\ell}) + \\alpha \\min_{c} \\hat \\pi_{tc}\\phi(Y_{it};\\hat\\mu_{tc},\\hat\\Sigma_{tc}) \\right)} \\] where \\(\\alpha_L\\) is the Laplace smoothing constant (which defaults to 0). Here is the function that implements this approach: #&#39; Initialize the Kernel EM-algorithm using Bayesian methods #&#39; &lt;&lt;y-param&gt;&gt; #&#39; @param K number of components #&#39; @param biomass list of length T, where each element `biomass[[t]]` is a #&#39; numeric vector of length n_t containing the biomass (or count) of particles #&#39; in each bin #&#39; @param num_iter number of iterations of EM to perform #need to think more #&#39; about iterations for Bayes init #&#39; @param lap_smooth_const Laplace smoothing constant #More explanation needed #&#39; @export init_bayes &lt;- function (y, K, biomass = default_biomass(y), num_iter = 1, lap_smooth_const = 0){ num_times &lt;- length(y) d &lt;- ncol(y[[1]]) resp &lt;- list() # responsibilities gamma[[t]][i, k] log_resp &lt;- list() resp_weighted &lt;- list() resp_sum &lt;- list() resp_sum_pi &lt;- list() y_sum &lt;- list() mat_sum &lt;- list() yy &lt;- list() mu &lt;- array(NA, c(num_times, K, d)) Sigma &lt;- array(NA, c(num_times, K, d, d)) pi &lt;- matrix(NA, num_times, K) if (d == 1){ init_fit &lt;- mclust::Mclust(y[[1]], G = K, modelNames = &quot;V&quot;) ii = 1 while (is.null(init_fit) == TRUE){ init_fit &lt;- mclust::Mclust(y[[1 + ii]], G = K, modelNames = &quot;V&quot;) ii = ii + 1 } mu[1, , 1] &lt;- init_fit$parameters$mean Sigma[1, , 1, 1] &lt;- init_fit$parameters$variance$sigmasq pi [1, ] &lt;- init_fit$parameters$pro } else if (d &gt; 1){ init_fit &lt;- mclust::Mclust(y[[1]], G = K, modelNames = &quot;VVV&quot;) ii = 1 while (is.null(init_fit) == TRUE){ init_fit &lt;- mclust::Mclust(y[[1 + ii]], G = K, modelNames = &quot;V&quot;) ii = ii + 1 } mu[1, ,] &lt;- t(init_fit$parameters$mean) pi[1, ] &lt;- init_fit$parameters$pro Sigma[1, , , ] &lt;- aperm(init_fit$parameters$variance$sigma, c(3,1,2)) } phi &lt;- matrix(NA, nrow(y[[1]]), K) log_phi &lt;- matrix(NA, nrow(y[[1]]), K) if (d == 1) { for (k in seq(K)) { log_phi[, k] &lt;- stats::dnorm(y[[1]], mean = mu[1, k, 1], sd = sqrt(Sigma[1, k, 1, 1]), log = TRUE) } } else if (d &gt; 1) { for (k in seq(K)) { log_phi[, k] &lt;- mvtnorm::dmvnorm(y[[1]], mean = mu[1, k, ], sigma = Sigma[1, k, , ], log = TRUE) } } log_temp = t(t(log_phi) + log(pi[1, ])) log_resp = log_temp - matrixStats::rowLogSumExps(log_temp) resp[[1]] = exp(log_resp) resp_weighted[[1]] = diag(biomass[[1]]) %*% resp[[1]] resp_sum[[1]] &lt;- colSums(resp_weighted[[1]]) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) pi_prev &lt;- pi mu_prev &lt;- mu Sigma_prev &lt;- Sigma for (tt in 2:num_times){ for (l in seq(num_iter)) { # E-step phi &lt;- matrix(NA, nrow(y[[tt]]), K) if (l == 1){ if (d == 1) { for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu_prev[tt - 1, k, 1], sd = sqrt(Sigma_prev[tt - 1, k, 1, 1])) } } else if (d &gt; 1) { for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu_prev[tt - 1, k, ], sigma = Sigma_prev[tt - 1, k, , ]) } } temp &lt;- t(t(phi) * pi_prev[tt - 1, ]) }else if (l &gt; 1) { if (d == 1) { for (k in seq(K)) { phi[, k] &lt;- stats::dnorm(y[[tt]], mean = mu_prev[tt, k, 1], sd = sqrt(Sigma_prev[tt, k, 1, 1])) } } else if (d &gt; 1) { for (k in seq(K)) { phi[, k] &lt;- mvtnorm::dmvnorm(y[[tt]], mean = mu_prev[tt, k, ], sigma = Sigma_prev[tt, k, , ]) } } temp &lt;- t(t(phi) * pi_prev[tt, ]) } temp_smooth = temp + lap_smooth_const * apply(temp, 1, min) resp[[tt]] &lt;- temp_smooth / rowSums(temp_smooth) resp_weighted[[tt]] = diag(biomass[[tt]]) %*% resp[[tt]] zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) #M-step #M-step pi resp_sum[[tt]] &lt;- colSums(resp_weighted[[tt]]) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) for (k in seq(K)){ pi[tt, k] &lt;- (resp_sum [[tt - 1]][, k] + resp_sum [[tt]][, k]) / (rowSums(resp_sum[[tt - 1]]) + rowSums(resp_sum[[tt]])) } #M-step mu y_sum[[tt]] &lt;- crossprod(resp_weighted[[tt]], y[[tt]]) %&gt;% unlist() %&gt;% array(c(K, d)) for (k in seq(K)){ mu[tt, k, ] &lt;- ((resp_sum[[tt - 1]][, k] * mu[tt - 1, k , ]) + y_sum[[tt]][k, ]) / (resp_sum[[tt - 1]][, k] + resp_sum[[tt]][, k]) } #M-step Sigma mat_sum[[tt]] &lt;- array(NA, c(K, d, d)) yy[[tt]] &lt;- matrix(NA, dim(y[[tt]])[1], d) for (k in seq(K)) { for(dd in seq(d)) { yy [[tt]][, dd] &lt;- (y[[tt]][, dd]- mu[tt, k, dd]) } mat_sum[[tt]][k, , ] &lt;- crossprod(yy[[tt]], yy[[tt]] * resp_weighted[[tt]][, k]) # YY^T * D * YY } for (k in seq(K)){ Sigma[tt, k, , ] &lt;- ((resp_sum[[tt - 1]][, k] * Sigma[tt - 1, k, , ]) + mat_sum[[tt]][k, , ]) / (resp_sum[[tt - 1]] [, k] + resp_sum[[tt]] [, k]) } pi_prev &lt;- pi mu_prev &lt;- mu Sigma_prev &lt;- Sigma } } dimnames(mu) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL) dimnames(Sigma) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K), NULL, NULL) dimnames(pi) &lt;- list(NULL, paste0(&quot;cluster&quot;, 1:K)) zest &lt;- resp %&gt;% purrr::map(~ max.col(.x)) fit_init = list(mu = mu, Sigma = Sigma, pi = pi, resp = resp, zest = zest) return(fit_init) } Here is what the Bayes initialization gives for the same example data: ex1_init_bayes = init_bayes(ex1$dat$y, K = 2) plot_data_and_model(ex1$dat$y, ex1_init_bayes$zest, ex1_init_bayes$mu) This is much better than the constant initialization! But it will be significantly more computationally expensive with real data. Here is the 3-d example: ex2_init_bayes = init_bayes(ex2$dat$y, K = 4) plot_data_and_model(ex2$dat$y, ex2_init_bayes$zest, ex2_init_bayes$mu) Let’s add the necessary packages to our package: usethis::use_package(&quot;mclust&quot;) usethis::use_package(&quot;matrixStats&quot;) ## ✔ Adding &#39;mclust&#39; to Imports field in DESCRIPTION ## • Refer to functions with `mclust::fun()` ## ✔ Adding &#39;matrixStats&#39; to Imports field in DESCRIPTION ## • Refer to functions with `matrixStats::fun()` Now that we have responsibilities, we can plot the biomass for each cluster over time: plot_biomass(ex2$dat$biomass, ex2_init_bayes$resp) The lines are so jagged because we independently drew from a normal distribution for the biomass of each “bin” at each time. 3.4 Trying out the method Let’s first try out our 1-d example, with all bandwidths equal to 5. Notice that no initialization is explicitly fed into the kernel_em() function, so it will use a constant initialization. fit &lt;- kernel_em(ex1$dat$y, K = 2, hmu = 5, hSigma = 5, hpi = 5, num_iter = 20) plot_data_and_model(ex1$dat$y, fit$zest, fit$mu) Now let’s try bandwidths equal to 50. fit &lt;- kernel_em(ex1$dat$y, K = 2, hmu = 50, hSigma = 50, hpi = 50, num_iter = 10, initial_fit = ex1_init_const) plot_data_and_model(ex1$dat$y, fit$zest, fit$mu) Now let’s try out our 3-d example, with bandwidths 5 again: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 5, hSigma = 5, hpi = 5, num_iter = 10, initial_fit = ex2_init_const) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time: plot_pi(fit$pi) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 1, show_data = FALSE) We can look at how the means evolve in any dimension with the data too: plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 2) Let’s have a look at the responsibilities that we get from our model. We consider the first 4 bins in the 50th time point: fit$resp[[50]][1:4, ] ## [,1] [,2] [,3] [,4] ## [1,] 2.844005e-24 1.283566e-16 2.284311e-08 1.000000e+00 ## [2,] 3.333286e-26 6.579083e-19 8.359285e-12 1.000000e+00 ## [3,] 1.673301e-34 1.000000e+00 8.566423e-13 5.070375e-23 ## [4,] 1.000000e+00 3.914394e-29 3.618382e-20 9.759917e-24 We see that the algorithm is pretty certain tabout it’s assignments, but we might not always have this certainty for points that are somewhat “midway” between different cluster centers. For example: round(fit$resp[[92]][135,],2) ## [1] 0 0 0 1 Now let’s see what happens when we use a much larger bandwidth: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 50, hSigma = 50, hpi = 50, num_iter = 10, initial_fit = ex2_init_const) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time again: plot_pi(fit$pi) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 1, show_data = FALSE) Now let’s see what happens when we use the Bayesian initialization: fit &lt;- kernel_em(ex2$dat$y, K = 4, hmu = 5, hSigma = 5, hpi = 5, num_iter = 10, initial_fit = ex2_init_bayes) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu) Let’s look at how the \\(\\pi\\)’s and \\(\\mu\\)’s evolve over time again: plot_pi(fit$pi) plot_data_and_model(ex2$dat$y, fit$zest, fit$mu, dim = 1, show_data = FALSE) They look pretty similar! The algorithm is able to make up the difference between the cheap constant initialization and the improved Bayesian initialization, at least in this simple case. 3.5 Cross-Validation We use cross validation to help with the selection of our bandwidths for the kernel smoother, implementing a 5-fold cross-validation scheme where each fold \\(\\mathcal{I}_1, \\mathcal{I}_2, \\mathcal{I}_3, \\mathcal{I}_4, \\mathcal{I}_5\\) constitutes every 5th time point, so \\(\\mathcal{I}_\\ell = \\{\\ell, \\ell + 5, \\ell + 10, \\dots\\}\\). We first fit the model on all folds apart from \\(\\mathcal{I}_\\ell\\), using \\(\\theta^{-\\mathcal{I}_\\ell}\\) to denote the collection of \\(\\gamma_{itk}\\), \\(\\pi_{tk}\\), \\(\\mu_{tk}\\), and \\(\\Sigma_{tk}\\) for all \\(t \\in \\mathcal{T} \\setminus \\mathcal{I}_\\ell\\), where \\(\\mathcal{T}\\) is the set of all time points in our data set. We then use our prediction function \\(\\hat{f}_{pred}(\\mathcal{I_\\ell}, \\theta^{-\\mathcal{I}_\\ell})\\), which takes in all the time points \\(\\mathcal{I}_\\ell\\) at which we need predictions as well as our fit \\(\\theta^{-\\mathcal{I}_\\ell}\\), and returns the parameters \\(\\mu_{kt},\\Sigma_{kt}\\), and \\(\\pi_{kt}\\) for all time points in \\(\\mathcal{I}_\\ell\\) using the M-step equations above. Finally, we evaluate the log-likelihood on these unseen time points \\(\\mathcal{I}_\\ell\\) using the predicted parameters from \\(\\hat{f}_{pred}(\\mathcal{I_\\ell}, \\theta^{-\\mathcal{I}_\\ell})\\), summing over all points at a given time and then averaging over all time points in \\(\\mathcal{I}_\\ell\\), to give us a single number for the fold. We then repeat this procedure for the remaining folds and take the average to give us a final cross-validation score for the current set of parameter values: \\[ \\text{CV} = \\frac{1}{5}\\sum_{\\ell=1}^{5} \\frac{1}{|\\mathcal{I}_\\ell|} \\sum_{t \\in \\mathcal{I}_\\ell} \\sum_{i \\in [n_t]} \\log\\mathcal{L}\\left(Y_{it} \\mid \\hat{f}_{\\text{pred}}\\left(\\mathcal{I}_\\ell, \\theta^{-\\mathcal{I}_\\ell}\\right)\\right) \\] In our case, we chose to vary \\(h_{\\mu}\\) and \\(h_{\\pi}\\) over a grid of values, while keeping \\(h_{\\Sigma}\\) fixed. Our grid values for \\(h_{\\mu}\\) and \\(h_{\\pi}\\) vary constantly on a logarithmic scale, starting at 1 and ending at the the total length of time for the data - this end point could be replaced with the first or second quartile of the total length. For our code, we begin with a function that predicts parameter values outside the set of observed time points using the M-step equations above. #&#39; @param fit A list returned by `kernel_em` containing the fitted parameters (mu, Sigma, pi, and responsibilities) from the training data. #&#39; @param test_dates A vector of POSIXct (or POSIXt) date-time objects of length T_test representing the time points at which predictions are required. #&#39; @param train_dates A vector of POSIXct (or POSIXt) date-time objects of length T_train representing the time points corresponding to the training data. #&#39; @param train_data A list of length T_train, where each element is a matrix containing the observations for the training set at the corresponding time point. #&#39; @param train_biomass A list of length T_train, where each element is a numeric vector of length n_t containing the biomass (or count) for the observations in the training set. #&#39; @param hmu Bandwidth parameter for smoothing the mu estimates. #&#39; @param hSigma Bandwidth parameter for smoothing the Sigma estimates. #&#39; @param hpi Bandwidth parameter for smoothing the pi (mixing proportions) estimates. #&#39; @param scale_dates Logical flag indicating whether to rescale the date-time objects. If TRUE, the dates are converted to numeric values and rescaled (by subtracting the minimum training date and dividing by 3600); if FALSE, the dates are used directly. Defaults to TRUE. #&#39; @export kernel_em_predict &lt;- function(fit, test_dates, train_dates, train_data, train_biomass, hmu, hSigma, hpi, scale_dates = TRUE) { num_test &lt;- length(test_dates) num_train &lt;- length(train_dates) K &lt;- ncol(fit$pi) d &lt;- dim(fit$mu)[3] mu &lt;- array(NA, c(num_test, K, d)) Sigma &lt;- array(NA, c(num_test, K, d, d)) pi &lt;- matrix(NA, num_test, K) # Convert to numeric numeric_train_dates &lt;- as.numeric(train_dates) numeric_test_dates &lt;- as.numeric(test_dates) if (scale_dates) { # If we are given real dates, then we do the original rescaling min_date &lt;- min(numeric_train_dates) rescaled_train_dates &lt;- (numeric_train_dates - min_date) / 3600 rescaled_test_dates &lt;- (numeric_test_dates - min_date) / 3600 } else { # If dates are already in hours or integer indices, treat them directly rescaled_train_dates &lt;- numeric_train_dates rescaled_test_dates &lt;- numeric_test_dates } # 1) Predict pi (mixing proportions) ---------------------------------------- # Weighted responsibilities (weights = biomass) resp_weighted &lt;- purrr::map2(train_biomass, fit$resp, ~ .y * .x) # Sum over each mixture component across all observations resp_sum &lt;- purrr::map(resp_weighted, ~ colSums(.x)) %&gt;% unlist() %&gt;% matrix(ncol = K, byrow = TRUE) # Smooth those sums across time resp_sum_smooth &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(rescaled_train_dates, x, kernel = &quot;normal&quot;, bandwidth = hpi, x.points = rescaled_test_dates)$y ) # Normalize to get pi pi &lt;- resp_sum_smooth / rowSums(resp_sum_smooth) # 2) M-step for mu --------------------------------------------------------- # Weighted sum of y&#39;s for each mixture component y_sum &lt;- purrr::map2(resp_weighted, train_data, ~ crossprod(.x, .y)) %&gt;% unlist() %&gt;% array(c(K, d, num_train)) %&gt;% aperm(c(3,1,2)) # Smooth each dimension across time y_sum_smoothed &lt;- apply( y_sum, 2:3, function(x) stats::ksmooth(rescaled_train_dates, x, kernel = &quot;normal&quot;, bandwidth = hmu, x.points = rescaled_test_dates)$y ) # Smooth the sum of responsibilities (again, but with bandwidth hmu) resp_sum_smooth_mu &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(rescaled_train_dates, x, kernel = &quot;normal&quot;, bandwidth = hmu, x.points = rescaled_test_dates)$y ) # Combine smoothed sums to get mu for (j in seq(d)) { mu[, , j] &lt;- y_sum_smoothed[, , j] / resp_sum_smooth_mu } # 3) M-step for Sigma ------------------------------------------------------ mat_sum &lt;- array(NA, c(num_train, K, d, d)) for (tt in seq(num_train)) { # Prepare a matrix for each observation&#39;s difference from mu yy &lt;- matrix(NA, nrow(train_data[[tt]]), d) for (k_idx in seq(K)) { for (dd in seq(d)) { yy[, dd] &lt;- train_data[[tt]][, dd] - fit$mu[tt, k_idx, dd] } mat_sum[tt, k_idx, , ] &lt;- crossprod(yy, yy * resp_weighted[[tt]][, k_idx]) } } # Smooth Sigma the same way mat_sum_smoothed &lt;- apply( mat_sum, 2:4, function(x) stats::ksmooth(rescaled_train_dates, x, kernel = &quot;normal&quot;, bandwidth = hSigma, x.points = rescaled_test_dates)$y ) resp_sum_smooth_Sigma &lt;- apply( resp_sum, 2, function(x) stats::ksmooth(rescaled_train_dates, x, kernel = &quot;normal&quot;, bandwidth = hSigma, x.points = rescaled_test_dates)$y ) for (j in seq(d)) { for (l in seq(d)) { Sigma[, , j, l] &lt;- mat_sum_smoothed[, , j, l] / resp_sum_smooth_Sigma } } list(mu = mu, Sigma = Sigma, pi = pi) } We then have a function to compute the log-likelihood: #&#39; Compute the Average Log-Likelihood for Predicted Parameters #&#39; #&#39; This function computes the average log-likelihood of the observed data over all time points using the predicted parameters. #&#39; #&#39; @param y A list of length T, where each element is a matrix of observations at a given time point. Each matrix should have dimensions corresponding to the number of observations at that time point by the data dimension. #&#39; @param pred A list containing the predicted parameters from the model. It should include: #&#39; \\describe{ #&#39; \\item{mu}{An array of predicted means with dimensions [T, K, d], where T is the number of time points, K is the number of clusters, and d is the data dimension.} #&#39; \\item{Sigma}{An array of predicted covariance matrices with dimensions [T, K, d, d].} #&#39; \\item{pi}{A matrix of predicted mixing proportions with dimensions [T, K].} #&#39; } #&#39; #&#39; @return A numeric value representing the average log-likelihood over all time points. #&#39; @export compute_log_like &lt;- function(y, pred) { d &lt;- ncol(y[[1]]) # Dimension of the data ntimes &lt;- length(y) # Number of time points K &lt;- dim(pred$pi)[2] # Number of clusters log_like &lt;- numeric(ntimes) for (tt in seq(ntimes)) { likelihood_tt &lt;- 0 for (k in seq(K)) { if (d == 1) { # Univariate case likelihood_k &lt;- stats::dnorm(y[[tt]], mean = pred$mu[tt, k, 1], sd = sqrt(pred$Sigma[tt, k, 1, 1])) } else { # Multivariate case likelihood_k &lt;- mvtnorm::dmvnorm(y[[tt]], mean = pred$mu[tt, k, ], sigma = pred$Sigma[tt, k, , ]) } # Weighted likelihood with mixing proportions likelihood_tt &lt;- likelihood_tt + pred$pi[tt, k] * likelihood_k } log_like[tt] &lt;- sum(log(likelihood_tt)) } return(mean(log_like)) } We have a small function to help ensure objects remain as matrices: #&#39; Ensure an Object is a Matrix #&#39; #&#39; This helper function checks whether the provided object is a matrix and, if it is not, converts it to a matrix using \\code{as.matrix()}. #&#39; #&#39; @param x An object that is expected to be a matrix. If \\code{x} is not already a matrix, it will be coerced into one. #&#39; #&#39; @return A matrix. If the input was not a matrix, it is converted using \\code{as.matrix()}. #&#39; #&#39; @examples #&#39; # Example 1: x is a vector #&#39; vec &lt;- c(1, 2, 3) #&#39; mat &lt;- ensure_matrix(vec) #&#39; is.matrix(mat) # Should return TRUE #&#39; #&#39; # Example 2: x is already a matrix #&#39; mat2 &lt;- matrix(1:4, nrow = 2) #&#39; identical(ensure_matrix(mat2), mat2) # Should return TRUE #&#39; #&#39; @export ensure_matrix &lt;- function(x) { if (!is.matrix(x)) x &lt;- as.matrix(x) return(x) } And now we have the function that computes the cross-validation score given a certain set of bandwidths: #&#39; Cross-Validated Log-Likelihood for Kernel EM Model #&#39; #&#39; This function performs K-fold cross-validation (by default, 5-fold) for selecting the bandwidth parameters in a kernel-based EM model. The data is partitioned into folds by leaving out every \\code{leave_out_every}-th time point, where the \\eqn{\\ell}-th fold is given by #&#39; \\deqn{\\mathcal{I}_\\ell = \\{\\ell, \\ell+ \\text{leave_out_every}, \\ell+2 \\times \\text{leave_out_every}, \\dots\\}.} #&#39; For each fold, the model is fit on the remaining time points using \\code{kernel_em} and predictions for the left-out points are generated using \\code{kernel_em_predict}. The log-likelihood of the left-out data is computed using \\code{compute_log_like}, and the final cross-validation score is the average log-likelihood over all folds. #&#39; #&#39; @param y A list of length T, where each element is a matrix of observations at a given time point. #&#39; @param K Number of mixture components (clusters) in the model. #&#39; @param dates A vector of date-time objects (e.g., of class \\code{POSIXct}) of length T corresponding to the time points in \\code{y}. #&#39; @param hmu Bandwidth parameter for smoothing the \\code{mu} (mean) estimates. #&#39; @param hSigma Bandwidth parameter for smoothing the \\code{Sigma} (covariance) estimates. #&#39; @param hpi Bandwidth parameter for smoothing the \\code{pi} (mixing proportions) estimates. #&#39; @param biomass A list of length T, where each element is a numeric vector of length \\eqn{n_t} containing the biomass (or count) of particles for the observations at that time point. #&#39; @param leave_out_every The number of folds for cross-validation (default is 5). The \\eqn{\\ell}-th fold is defined as the set of time points \\eqn{\\{\\ell, \\ell + \\text{leave_out_every}, \\ell + 2 \\times \\text{leave_out_every}, \\dots\\}}. #&#39; @param scale_dates Logical flag indicating whether to rescale the date-time objects. If \\code{TRUE}, the dates are converted to numeric values (by subtracting the minimum date and dividing by 3600) before smoothing; if \\code{FALSE}, the dates are used directly. #&#39; #&#39; @return A numeric value representing the average log-likelihood over all cross-validation folds. #&#39; #&#39; @details For each fold, the model is fit using the training data (all time points not in the current fold) and then predictions are made for the left-out time points. The log-likelihood for each fold is computed by summing the log-likelihoods over all observations and then averaging over the time points in the fold. The overall cross-validation score is obtained by averaging the log-likelihoods over all folds. #&#39; @export kernel_em_cv &lt;- function(y, K, dates, hmu, hSigma, hpi, biomass, leave_out_every = 5, scale_dates = TRUE) { n &lt;- length(y) if (leave_out_every &lt;= 0 || leave_out_every &gt;= n) { stop(&quot;leave_out_every must be a positive integer smaller than the number of data points&quot;) } pred &lt;- vector(&quot;list&quot;, leave_out_every) log_like &lt;- vector(&quot;list&quot;, leave_out_every) for (i in seq_len(leave_out_every)) { test_indices &lt;- seq(i, n, by = leave_out_every) train_data &lt;- lapply(y[-test_indices], ensure_matrix) train_date &lt;- dates[-test_indices] train_biomass &lt;- biomass[-test_indices] test_data &lt;- lapply(y[test_indices], ensure_matrix) test_date &lt;- dates[test_indices] test_biomass &lt;- biomass[test_indices] if (scale_dates){ fit_train &lt;- kernel_em(y = train_data, K = K, hmu = hmu, hSigma = hSigma, hpi = hpi, dates = train_date, biomass = train_biomass) } else { fit_train &lt;- kernel_em(y = train_data, K = K, hmu = hmu, hSigma = hSigma, hpi = hpi, biomass = train_biomass) } pred[[i]] &lt;- kernel_em_predict(fit = fit_train, test_dates = test_date, train_dates = train_date, train_data = train_data, train_biomass = train_biomass, hmu = hmu, hSigma = hSigma, hpi = hpi, scale_dates = scale_dates) log_like[[i]] &lt;- compute_log_like(test_data, pred[[i]]) } avg_log_likelihood &lt;- mean(unlist(log_like)) return(avg_log_likelihood) } Finally, we have the function that performs cross validation over a grid of parameter values, where \\(h_{\\mu}\\) and \\(h_{\\pi}\\) vary, and \\(h_{\\Sigma}\\) is a fixed value. Let’s first add the packages we used to make CV run in parallel: usethis::use_package(&quot;foreach&quot;) usethis::use_package(&quot;doParallel&quot;) usethis::use_package(&quot;parallel&quot;) usethis::use_import_from(&quot;foreach&quot;, c(&quot;foreach&quot;, &quot;%dopar%&quot;)) ## ✔ Adding &#39;foreach&#39; to Imports field in DESCRIPTION ## • Refer to functions with `foreach::fun()` ## ✔ Adding &#39;doParallel&#39; to Imports field in DESCRIPTION ## • Refer to functions with `doParallel::fun()` ## ✔ Adding &#39;parallel&#39; to Imports field in DESCRIPTION ## • Refer to functions with `parallel::fun()` ## ✔ Adding &#39;@importFrom foreach foreach&#39;, &#39;@importFrom foreach %dopar%&#39; to &#39;R/flowkernel-package.R&#39; ## ✔ Writing &#39;NAMESPACE&#39; #&#39; Grid Search Cross-Validation for Kernel EM Model #&#39; #&#39; This function performs a grid search over bandwidth parameters for the kernel-based EM model via cross-validation. #&#39; The grid search is performed over the \\code{hmu} (for the mean parameter) and \\code{hpi} (for the mixing proportions) #&#39; while keeping \\code{hSigma} fixed. Cross-validation is implemented by partitioning the data into folds by leaving out every #&#39; \\code{leave_out_every}-th time point. For each parameter combination, the model is fit on the training data and evaluated #&#39; on the left-out data using the log-likelihood. The best parameters are selected as those that maximize the cross-validation score. #&#39; #&#39; @param y A list of length T, where each element is a matrix of observations at a given time point. #&#39; @param K Number of mixture components (clusters) in the model. #&#39; @param hSigma Bandwidth parameter for smoothing the \\code{Sigma} (covariance) estimates. #&#39; @param dates A vector of date-time objects (e.g., of class \\code{POSIXct}) of length T corresponding to the time points in \\code{y}. #&#39; If \\code{NULL}, the function uses the sequence of indices. #&#39; @param biomass A list of length T, where each element is a numeric vector of length \\eqn{n_t} containing the biomass (or count) #&#39; for the observations at that time point. Defaults to \\code{default_biomass(y)}. #&#39; @param leave_out_every The number of folds for cross-validation (default is 5). The \\eqn{\\ell}-th fold is defined as the set of #&#39; time points \\eqn{\\{\\ell, \\ell + \\text{leave_out_every}, \\ell + 2 \\times \\text{leave_out_every}, \\dots\\}}. #&#39; @param grid_size Number of grid points for each bandwidth parameter, resulting in a x grid. Defaults to 7 #&#39; @param ncores Number of cores to use for parallel processing. If \\code{NULL} or less than 2, the grid search is performed sequentially. #&#39; @param log_grid Logical flag indicating whether the grid for the bandwidth parameters should be logarithmically spaced (if \\code{TRUE}) #&#39; or linearly spaced (if \\code{FALSE}). Defaults to \\code{TRUE}. #&#39; @param grid_length_half Logical flag indicating whether to use half the total time length (or median time) for computing the end of the grid. #&#39; #&#39; @return A list containing: #&#39; \\describe{ #&#39; \\item{results}{A data frame (converted from a matrix) of cross-validation scores, with row names corresponding to evaluated \\code{hmu} values #&#39; and column names to evaluated \\code{hpi} values.} #&#39; \\item{best_hmu}{The best \\code{hmu} value (bandwidth for smoothing \\code{mu}) according to the highest CV score.} #&#39; \\item{best_hpi}{The best \\code{hpi} value (bandwidth for smoothing \\code{pi}) according to the highest CV score.} #&#39; \\item{max_cv_score}{The highest cross-validation score observed.} #&#39; \\item{hmu_vals}{The vector of \\code{hmu} values that were evaluated.} #&#39; \\item{hpi_vals}{The vector of \\code{hpi} values that were evaluated.} #&#39; \\item{time_elapsed}{The total time elapsed (in seconds) during the grid search.} #&#39; } #&#39; #&#39; @details The function first determines whether the dates should be scaled based on whether \\code{dates} is provided. #&#39; A grid of bandwidth values is then constructed (either logarithmically or linearly spaced) from the calculated range. #&#39; Cross-validation is performed for each parameter combination via the \\code{kernel_em_cv} function. The grid search can be run #&#39; either sequentially or in parallel, depending on the value of \\code{ncores}. #&#39; #&#39; @export cv_grid_search &lt;- function(y, K, hSigma = 10, dates = NULL, biomass = default_biomass(y), leave_out_every = 5, grid_size = 7, ncores = NULL, log_grid = TRUE, grid_length_half = TRUE) { start_time &lt;- Sys.time() # Decide if we are scaling dates or not if (is.null(dates)) { numeric_dates &lt;- seq_along(y) scale_dates &lt;- FALSE starting_date &lt;- min(numeric_dates) middle_date &lt;- stats::median(numeric_dates) if (grid_length_half) { end_cv &lt;- middle_date - starting_date } else { end_cv &lt;- max(numeric_dates) - starting_date } } else { numeric_dates &lt;- as.numeric(dates) scale_dates &lt;- TRUE starting_date &lt;- min(numeric_dates) middle_date &lt;- stats::median(numeric_dates) if (grid_length_half) { end_cv &lt;- (middle_date - starting_date) / 3600 } else { end_cv &lt;- (max(numeric_dates) - starting_date) / 3600 } } if (log_grid) { # LOG-SPACED GRID h_values &lt;- exp(seq(log(end_cv), log(2), length = grid_size)) } else { # LINEARLY-SPACED GRID h_values &lt;- seq(end_cv, 2, length.out = grid_size) } h_values &lt;- round(h_values) h_values &lt;- unique(h_values) h_values &lt;- h_values[h_values &gt; 0] hmu_vals &lt;- h_values hpi_vals &lt;- h_values # Create a data frame of all parameter combinations param_grid &lt;- expand.grid(hmu = hmu_vals, hpi = hpi_vals) # Decide whether to run in parallel or sequentially if (is.null(ncores) || as.numeric(ncores) &lt; 2) { message(&quot;Running sequentially...&quot;) # Prepare an empty data frame to store results cv_scores &lt;- data.frame(hmu = numeric(0), hpi = numeric(0), cv_score = numeric(0)) total &lt;- nrow(param_grid) for (i in seq_len(nrow(param_grid))) { hmu &lt;- param_grid$hmu[i] hpi &lt;- param_grid$hpi[i] processed &lt;- i remaining &lt;- total - i # Print detailed progress information message(&quot;Evaluating parameter combination &quot;, processed, &quot; of &quot;, total, &quot;: hmu = &quot;, hmu, &quot;, hpi = &quot;, hpi, &quot;. Remaining: &quot;, remaining) cv_score &lt;- kernel_em_cv( y = y, K = K, dates = numeric_dates, hmu = hmu, hSigma = hSigma, hpi = hpi, biomass = biomass, leave_out_every = leave_out_every, scale_dates = scale_dates ) cv_scores &lt;- rbind(cv_scores, data.frame(hmu = hmu, hpi = hpi, cv_score = cv_score)) } } else { message(&quot;Running in parallel on &quot;, ncores, &quot; cores...&quot;) cl &lt;- parallel::makeCluster(as.integer(ncores)) doParallel::registerDoParallel(cl) # Export needed variables/functions parallel::clusterExport( cl, varlist = c(&quot;kernel_em_cv&quot;, &quot;ensure_matrix&quot;, &quot;kernel_em&quot;, &quot;kernel_em_predict&quot;, &quot;kernel_em_predict&quot;, &quot;calculate_responsibilities&quot;, &quot;compute_log_like&quot;, &quot;y&quot;, &quot;K&quot;, &quot;hSigma&quot;, &quot;biomass&quot;, &quot;leave_out_every&quot;, &quot;numeric_dates&quot;, &quot;scale_dates&quot;), envir = environment() ) # Load required packages on the workers parallel::clusterEvalQ(cl, { library(flowkernel) }) cv_scores &lt;- foreach::foreach(i = seq_len(nrow(param_grid)), .combine = rbind) %dopar% { hmu &lt;- param_grid$hmu[i] hpi &lt;- param_grid$hpi[i] cv_score &lt;- kernel_em_cv( y = y, K = K, dates = numeric_dates, hmu = hmu, hSigma = hSigma, hpi = hpi, biomass = biomass, leave_out_every = leave_out_every, scale_dates = scale_dates ) data.frame(hmu = hmu, hpi = hpi, cv_score = cv_score) } parallel::stopCluster(cl) } # Convert cv_scores to matrix form if (nrow(cv_scores) == 0) { stop(&quot;No cross-validation scores were computed.&quot;) } results &lt;- reshape2::acast(cv_scores, hmu ~ hpi, value.var = &quot;cv_score&quot;) results_df &lt;- as.data.frame(results) # Re-label columns and rows for clarity colnames(results_df) &lt;- paste0(&quot;hpi_&quot;, colnames(results_df)) rownames(results_df) &lt;- paste0(&quot;hmu_&quot;, rownames(results_df)) # Identify best hmu/hpi max_pos &lt;- which(results == max(results, na.rm = TRUE), arr.ind = TRUE) best_hmu &lt;- as.numeric(rownames(results)[max_pos[1]]) best_hpi &lt;- as.numeric(colnames(results)[max_pos[2]]) # Timing end_time &lt;- Sys.time() run_time &lt;- as.numeric(difftime(end_time, start_time, units = &quot;secs&quot;)) hours &lt;- floor(run_time / 3600) minutes &lt;- floor((run_time %% 3600) / 60) seconds &lt;- round(run_time %% 60) cat(&quot;Best hmu:&quot;, best_hmu, &quot;\\nBest hpi:&quot;, best_hpi, &quot;\\nHighest CV score:&quot;, max(results, na.rm = TRUE), &quot;\\nTime Elapsed:&quot;, run_time, &quot;seconds =&gt;&quot;, hours, &quot;hours&quot;, minutes, &quot;minutes&quot;, seconds, &quot;seconds\\n&quot;) results_list &lt;- list( results = results_df, best_hmu = best_hmu, best_hpi = best_hpi, max_cv_score = max(results, na.rm = TRUE), hmu_vals = hmu_vals, hpi_vals = hpi_vals, time_elapsed = run_time ) return(results_list) } Let us try running this on our example data, using sequential processing. To run in parallel, we would choose a value of ncores greater than 1. ex1_cv_log_grid_results &lt;- cv_grid_search(y = ex1$dat$y, K = 2, hSigma = 10, biomass = ex1$dat$biomass, leave_out_every = 5, grid_size = 10, ncores = 1) ## Best hmu: 3 ## Best hpi: 42 ## Highest CV score: -26.70831 ## Time Elapsed: 131.9058 seconds =&gt; 0 hours 2 minutes 12 seconds We can examine the cross-validation scores for each set of parameter values by calling the following: ex1_cv_log_grid_results$results ## hpi_2 hpi_3 hpi_5 hpi_7 hpi_11 hpi_18 hpi_27 ## hmu_2 -27.09489 -27.01669 -26.91560 -26.88757 -26.85268 -26.83837 -26.83041 ## hmu_3 -26.98762 -26.90779 -26.80850 -26.77524 -26.75541 -26.73323 -26.72034 ## hmu_5 -27.30078 -27.21382 -27.11903 -27.08647 -27.05803 -27.03449 -27.01715 ## hmu_7 -28.20629 -28.12346 -28.01904 -27.99013 -27.95705 -27.93272 -27.91270 ## hmu_11 -32.05725 -31.99040 -31.87798 -31.84306 -31.80663 -31.77533 -31.74727 ## hmu_18 -40.23078 -40.14943 -40.05070 -40.01678 -39.97448 -39.94233 -39.92363 ## hmu_27 -45.52849 -45.45158 -45.34949 -45.31565 -45.27738 -45.24562 -45.23102 ## hmu_42 -46.45770 -46.37863 -46.29053 -46.26601 -46.23484 -46.22068 -46.21731 ## hmu_64 -46.46337 -46.38473 -46.29238 -46.26214 -46.23645 -46.21680 -46.20546 ## hmu_99 -46.45937 -46.38518 -46.28955 -46.26009 -46.23259 -46.21029 -46.20004 ## hpi_42 hpi_64 hpi_99 ## hmu_2 -26.81053 -26.81844 -26.92325 ## hmu_3 -26.70831 -26.71402 -26.80638 ## hmu_5 -27.00950 -27.03010 -27.12573 ## hmu_7 -27.90229 -27.91832 -28.03440 ## hmu_11 -31.73718 -31.76346 -31.87712 ## hmu_18 -39.91329 -39.94013 -40.08379 ## hmu_27 -45.22119 -45.22799 -45.32549 ## hmu_42 -46.21370 -46.22315 -46.32529 ## hmu_64 -46.20029 -46.21817 -46.31974 ## hmu_99 -46.19502 -46.21597 -46.32889 Note that all the scores are negative because they are log-likelihoods. The higher the score (and so the less negative), the better. Let’s add a function to plot these CV results: #&#39; Plot Cross-Validation Results #&#39; #&#39; This function generates an interactive 3D scatter plot of cross-validation results using Plotly. #&#39; It accepts a results list returned by \\code{cv_grid_search} and visualizes the grid of cross-validation #&#39; scores along with the evaluated bandwidth values (\\code{hmu} and \\code{hpi}). #&#39; #&#39; @param results_list A list returned by \\code{cv_grid_search}. This list should include: #&#39; \\describe{ #&#39; \\item{results}{A data frame of cross-validation scores, where the row names correspond to evaluated \\code{hmu} values and the column names to evaluated \\code{hpi} values.} #&#39; \\item{hmu_vals}{A numeric vector of the \\code{hmu} bandwidth values that were evaluated.} #&#39; \\item{hpi_vals}{A numeric vector of the \\code{hpi} bandwidth values that were evaluated.} #&#39; } #&#39; #&#39; @return A Plotly object representing a 3D scatter plot with: #&#39; \\describe{ #&#39; \\item{x-axis}{Evaluated \\code{hmu} bandwidth values.} #&#39; \\item{y-axis}{Evaluated \\code{hpi} bandwidth values.} #&#39; \\item{z-axis}{Corresponding cross-validation scores.} #&#39; } #&#39; @export plot_cv_results &lt;- function(results_list) { # Pull out the results results_df &lt;- results_list$results # Convert row/column names from strings (&quot;hmu_99&quot;) to numeric (99) row_nums &lt;- as.numeric(sub(&quot;hmu_&quot;, &quot;&quot;, rownames(results_df))) col_nums &lt;- as.numeric(sub(&quot;hpi_&quot;, &quot;&quot;, colnames(results_df))) # Sort the numeric row and column names row_sorted &lt;- sort(row_nums) col_sorted &lt;- sort(col_nums) # Create a new, reordered matrix # match() finds the indices of row_nums that correspond to row_sorted, etc. row_order &lt;- match(row_sorted, row_nums) col_order &lt;- match(col_sorted, col_nums) # Reorder results_df so row i truly corresponds to row_sorted[i], etc. results_mat &lt;- as.matrix(results_df[row_order, col_order]) # Flatten in column-major order cv_scores &lt;- c(results_mat) # Build a data frame matching the sorted bandwidths to their CV scores df &lt;- expand.grid(hmu = row_sorted, hpi = col_sorted) df$cv_score &lt;- cv_scores # Plot in 3D plotly::plot_ly( data = df, x = ~hmu, y = ~hpi, z = ~cv_score, type = &quot;scatter3d&quot;, mode = &quot;markers&quot; ) %&gt;% plotly::layout( title = &quot;Cross-Validation Results&quot;, scene = list( xaxis = list(title = &quot;hmu&quot;), yaxis = list(title = &quot;hpi&quot;), zaxis = list(title = &quot;CV Score&quot;) ) ) } Let’s take a look at the plot: fig &lt;- plot_cv_results(ex1_cv_log_grid_results) fig "],["conclude.html", "4 Conclusion", " 4 Conclusion When you are done defining the package, it remains to convert the Roxygen to documentation. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowkernel documentation ## ℹ Loading flowkernel ## ✖ In topic &#39;kernel_em_predict.Rd&#39;: Skipping; no name and/or title. ## Writing &#39;NAMESPACE&#39; ## Writing &#39;calculate_responsibilities.Rd&#39; ## Writing &#39;compute_log_like.Rd&#39; ## Writing &#39;cv_grid_search.Rd&#39; ## Writing &#39;default_biomass.Rd&#39; ## Writing &#39;ensure_matrix.Rd&#39; ## Writing &#39;flowkernel-package.Rd&#39; ## Writing &#39;generate_smooth_gauss_mix.Rd&#39; ## Writing &#39;init_bayes.Rd&#39; ## Writing &#39;init_const.Rd&#39; ## Writing &#39;kernel_em.Rd&#39; ## Writing &#39;kernel_em_cv.Rd&#39; ## Writing &#39;plot_1d_means_triple.Rd&#39; ## Writing &#39;plot_1d_means_with_width.Rd&#39; ## Writing &#39;plot_biomass.Rd&#39; ## Writing &#39;plot_cv_results.Rd&#39; ## Writing &#39;plot_data.Rd&#39; ## Writing &#39;plot_data_and_model.Rd&#39; ## Writing &#39;plot_pi.Rd&#39; ## Writing &#39;pipe.Rd&#39; You can also add some extra things to your package here if you like, such as a README, some vignettes, a pkgdown site, etc. See here for an example of how to do this with litr. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
